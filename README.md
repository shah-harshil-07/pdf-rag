ğŸ“„ğŸ¤– RAG-based PDF Chatbot

A Retrieval-Augmented Generation (RAG) powered chatbot that allows users to upload a PDF and ask questions directly from its content.
The chatbot answers strictly based on the uploaded document, ensuring accurate, context-aware responses.

âœ¨ Features

ğŸ“¤ Upload any PDF document

ğŸ§  Automatically processes and â€œlearnsâ€ from the document

ğŸ” Semantic search using vector embeddings

ğŸ’¬ Chat with the document in natural language

âš¡ Async processing using queues for scalability

ğŸš« No hallucinations â€” answers are grounded in the document

ğŸ—ï¸ Tech Stack
Frontend

Next.js

Backend

Node.js

Express.js

AI & Data Processing

LangChain â€“ PDF loading, chunking & orchestration

OpenAI Embeddings & LLMs

Qdrant â€“ Vector database for semantic search

Infrastructure

BullMQ â€“ Background job processing

Redis â€“ Queue storage for BullMQ

ğŸ§  How It Works (RAG Flow)

User uploads a PDF file

PDF is parsed and split into chunks using LangChain

Each chunk is converted into embeddings via OpenAI

Embeddings are stored in Qdrant Vector DB

User asks a question

Relevant chunks are retrieved using vector similarity

LLM generates an answer using retrieved context

ğŸš€ Getting Started
Prerequisites

Node.js (v18+ recommended)

Redis (running locally or via Docker)

Qdrant (local or cloud)

OpenAI API Key

Installation
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name

Install dependencies
npm install

Environment Variables

Create a .env file in the root directory:

OPENAI_SECRET_KEY=your_openai_api_key
QDRANT_URL=http://localhost:6333
QDRANT_PORT=6333
REDIS_HOST=localhost
REDIS_PORT=6379

Running the Application
Start Redis & Qdrant

You can use Docker:

docker run -p 6333:6333 qdrant/qdrant
docker run -p 6379:6379 redis

Start Backend
npm run dev

Start Frontend
npm run dev

ğŸ“½ï¸ Demo

ğŸ“Œ Upload a PDF â†’ Ask questions â†’ Get precise answers
https://www.linkedin.com/feed/update/urn:li:ugcPost:7405906365300469760/
